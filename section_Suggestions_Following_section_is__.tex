\section{Suggestions}

Following section is description of three suggestions about future works.

\subsection{Changing alphabet size} \label{alphabet}
Till now size of the alphabet was four ($\Sigma = \{A,C,G,T\}$) but can increase size of our alphabet by coding two characters instead of one character. By coding two characters our alphabet would change to:

\begin{equation}
\nonumber
\Sigma = \{ AA, AC, AG, AT, \dots, TC, TG, TT \}
\end{equation}

Following coding schema imposes multiple changes on our previous schema:

\begin{enumerate}
\item Data would compress. For instance if previous that our read's length was 100 we had to save 100 coded for each character but now, we can save 50 codes.
\item The cost of compressing data in this fashion is increasing length our coding. If we code our alphabet with 3 bits. Now we are coding our alphabet with 5 bits.
\item In vertical storage layout, if we transfer same size of bits in each iteration, for instance 16 bits, we are transferring bigger portion of the read (instead of $16\%$ of the code we are transferring $32\%$ of the code.
\item Our filter's sensitivity will \emph{decrease}. In the first schema we could detect every substitution of, for instance, T to A, C and G. But now we are detecting all substitution of AT to rest of permutation of characters.
\end{enumerate}
 
\subsection{New filter}
With considering all above facts we can suggest a filter. In suggested filter we can just compare limited number of highest bit of our reads and filter out big portion of them without doing \emph{full computation} and without \emph{transferring large portion} of our data. For tuning our filter we need to find a good balance point between following parameters:

\begin{enumerate}
\item Data level parallelism. How many reads we are going to compare in a same time? Our word processor's size is limited but we can choose number of reads we are going to load into a word processor with varying number of \underline{bits per reads}.
\item Coding and Compression. How large is our alphabet? According to \ref{alphabet} section there is a trade of between last two mentioned points. Are we going to have a better detector by having \underline{smaller alphabet} or we will have weaker detector but instead we are going to look at \underline{bigger portion of the code} with bigger alphabet size?
\end{enumerate}

\subsection{Prefetching}
In \ref{stride} section we suggested a secondary structure which helps us to predict data going to use in each iteration with computing distances of valid data from specific point of memory. Our suggested solution for this approach is improving memory controller and adding better prefetching capability to it. In Impulse \cite{impuls} similar approach is using for better prefetching.
  
  
  